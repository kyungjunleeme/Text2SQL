Metadata-Version: 2.4
Name: deepeval-text2sql-demo
Version: 0.3.20
Summary: DeepEval-based Text-to-SQL evaluation with SQLite/Trino/Databricks/Spark, Spider2 prep, dbt, and pytest.
Author-email: Kyungjun-ready <example@example.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: deepeval>=0.20.0
Requires-Dist: sqlalchemy>=2.0.0
Requires-Dist: pandas>=2.2.0
Requires-Dist: sqlglot>=25.0.0
Requires-Dist: click>=8.1.7
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: trino[sqlalchemy]>=0.330.0
Requires-Dist: databricks-sql-connector>=4.0.0
Requires-Dist: pytest>=8.0.0
Requires-Dist: dbt-core>=1.7.0
Requires-Dist: dbt-sqlite>=1.7.0
Requires-Dist: dbt-duckdb>=1.7.0
Provides-Extra: trino
Requires-Dist: trino[sqlalchemy]>=0.330.0; extra == "trino"
Provides-Extra: databricks
Requires-Dist: databricks-sql-connector>=3.3.1; extra == "databricks"
Provides-Extra: spark
Requires-Dist: pyhive>=0.7.0; extra == "spark"
Requires-Dist: thrift>=0.20.0; extra == "spark"
Requires-Dist: thrift-sasl>=0.4.3; extra == "spark"
Requires-Dist: pure-sasl>=0.6.2; extra == "spark"
Requires-Dist: thrift>=0.20.0; extra == "spark"
Requires-Dist: thrift-sasl>=0.4.3; extra == "spark"

# Text2SQL

[![CI](https://github.com/kyungjunleeme/Text2SQL/actions/workflows/ci.yml/badge.svg)](https://github.com/kyungjunleeme/Text2SQL/actions/workflows/ci.yml) (SQLite / Trino / Databricks / Spark) — with dbt & pytest

Text-to-SQL 모델을 **DeepEval**로 평가하는 레퍼런스 레포입니다.  
- 백엔드: SQLite(기본) / Trino / Databricks(SQL Warehouse) / Spark Thrift Server(PyHive)  
- 메트릭: 실행 가능성, 실행 결과 일치, 파서 기반 의미 비교, **컴포넌트 부분 채점**  
- Spider2 연동: lite/snow/DBT → `testcases.json` + (옵션) SQLite 스키마/DB 생성  
- **dbt 통합**: `dbt-sqlite` 프로파일로 `data/sample.db` 위에서 모델 빌드  
- **pytest**: 메트릭 단위 테스트 제공

## Quickstart
```bash
make uv-setup
make db-setup
make eval-sqlite
```

## dbt
```bash
# dbt 모델 빌드 (dbt-sqlite, data/sample.db 사용)
make dbt-build
# 또는
DBT_PROFILES_DIR=./dbt uv run dbt build --project-dir dbt
```

## Pytest
```bash
make test
```

## Spider2 연동 (lite/snow/DBT)
- json/jsonl → `tools/spider2_prepare.py`
- DBT-style 디렉토리 → `tools/spider2_dbt_prepare.py`
- lite/snow + 스키마/CSV → `tools/spider2_lite_snow_prepare.py` + `tools/build_sqlite_from_schema.py`

## GitHub 업로드
```bash
make git-init
make github-push GIT_REMOTE=git@github.com:YOUR/REPO.git
```

## CI 비밀값(선택) — 실 환경 평가
GitHub Actions에서 다음 **Secrets**를 설정하면 해당 단계가 자동 실행됩니다(없으면 스킵).
- `TRINO_ENGINE_URL` — 예: `trino://user@host:8080/hive/default`
- `DATABRICKS_ENGINE_URL` — 예: `databricks+connector://token:...@HOST:443/DEFAULT?http_path=/sql/1.0/warehouses/WH_ID&catalog=main&schema=default`
- `SPARK_HOST`, `SPARK_PORT`, `SPARK_DB`, `SPARK_AUTH`, `SPARK_USER` — Spark Thrift Server 접속 정보

로컬에서 CI 유사 실행:
```bash
make ci-local
```


### Python 3.13 호환성 메모
일부 서드파티 드라이버가 아직 Python 3.13 휠을 제공하지 않을 수 있어요. 기본 의존성은 3.13에서 동작하도록 구성했고,
Trino / Spark / Databricks 드라이버는 **옵션 의존성(extras)** 로 분리했습니다.

- Trino: `uv sync --extra trino`
- Spark(Thrift Server): `uv sync --extra spark`
- Databricks(SQL Warehouse): `uv sync --extra databricks`

> 만약 특정 드라이버가 3.13에서 문제가 있으면 `uv python install 3.12 && uv venv -p 3.12 && uv sync` 로 3.12 가상환경을 사용하세요.


### macOS/arm64에서 `sasl` 빌드 에러 회피
Spark Thrift Server를 **SASL 없이(auth=NONE)** 쓸 경우, C 확장인 `sasl` 패키지가 필요하지 않습니다.
본 레포의 `spark` extras는 **`pyhive` + `thrift` + `thrift-sasl` + `pure-sasl`** 로 구성되어 `sasl` (C 확장)을 설치하지 않습니다.

```bash
# 선택 설치(여러 개 가능): Trino + Spark + Databricks
uv sync --extra trino --extra spark --extra databricks

# 또는 Makefile로
make uv-setup EXTRAS="trino spark databricks"
```
> Kerberos/LDAP 등 실제 SASL이 필요한 환경이라면, OS 패키지(`cyrus-sasl` 등) 설치 후 `sasl` 파이썬 패키지를 별도 설치해야 합니다.


- https://www.databricks.com/blog/improving-text2sql-performance-ease-databricks?utm_source=chatgpt.com
- https://www.databricks.com/blog/unlocking-financial-insights-nyse-ice?utm_source=chatgpt.com
